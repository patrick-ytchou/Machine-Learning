{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "sets = ['train', 'val']\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                         data_transforms[x])\n",
    "                 for x in sets}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in sets}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in sets}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, n_epochs=3):\n",
    "    \"\"\"\n",
    "    1. train # optimizer.step()\n",
    "    2. evaluate\n",
    "    3. scheduler # scheduler.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print('-'*10)\n",
    "        \n",
    "        # Each epoch has a training and a validation phase\n",
    "        for phase in sets:\n",
    "            if phase == 'train':\n",
    "                model.train() # Set model to training mode\n",
    "            else:\n",
    "                model.eval() # Set model to evaluating mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Interate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # backward\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
    "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "train Loss: 0.6584 Acc: 0.5984\n",
      "val Loss: 0.4846 Acc: 0.8105\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "train Loss: 0.5219 Acc: 0.7500\n",
      "val Loss: 0.4062 Acc: 0.8366\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "train Loss: 0.4545 Acc: 0.7910\n",
      "val Loss: 0.3044 Acc: 0.9281\n",
      "\n",
      "Training complete in 2m 43s\n",
      "Best val Acc: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# Import pre-trained model to utilize transfer learning\n",
    "\n",
    "# Option 1\n",
    "# Fine-tuning all the weights based on the data\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2) # Change the last layer's output ftrs\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "# lr schedular\n",
    "# Every {step_size} epoch, the learning rate is multiplied by {gamma} (decay factor)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                        step_size=7,\n",
    "                                        gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, n_epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "train Loss: 0.6543 Acc: 0.6270\n",
      "val Loss: 0.5885 Acc: 0.7320\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "train Loss: 0.5730 Acc: 0.7172\n",
      "val Loss: 0.5284 Acc: 0.7451\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "train Loss: 0.5629 Acc: 0.7295\n",
      "val Loss: 0.4282 Acc: 0.7778\n",
      "\n",
      "Training complete in 1m 14s\n",
      "Best val Acc: 0.7778\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "# Freeze all the layers in the beginning and only train the very last layer\n",
    "# Faster than the first option\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # Freeze all the layers in the beginning\n",
    "    \n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2) # Set a new last layer, defaulting requires_grad = True\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "# lr schedular\n",
    "# Every {step_size} epoch, the learning rate is multiplied by {gamma} (decay factor)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                        step_size=7,\n",
    "                                        gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, n_epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two results above we can see that freezing all the parameters except those of the last layer can make the training much faster, but the accuracy increases much slower. The **torchvision.models** package provides lots of pre-trained models that are confirmed powerful in lots of papers in numerous tasks. Utilizing those models with transfer learning is a great idea to quickly put your idea into practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
