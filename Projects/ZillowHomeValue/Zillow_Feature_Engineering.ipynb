{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Feature Engineering\n",
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import warnings\n",
    "import time\n",
    "import numba as nb\n",
    "\n",
    "# display set up\n",
    "%precision 4\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer decorator\n",
    "@nb.jit()\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('----------------------')\n",
    "            print('Function %r takes %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for memory reduction\n",
    "@nb.jit()\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_null(series):\n",
    "    print('Total number of null values: ' + str(series.isnull().sum()))\n",
    "    print('Consist of ' + str(100*np.round(series.isnull().sum() / len(series),5)) + '% of the dataset. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = reduce_mem_usage(df)\n",
    "    print(f\"Number of rows: {len(df)}\".format(len(df)))\n",
    "    print(f\"Number of columns: {len(df.columns)-1} \\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  1.21 Mb (41.7% reduction)\n",
      "Number of rows: 90275\n",
      "Number of columns: 2 \n",
      "\n",
      "Mem. usage decreased to  1.04 Mb (41.7% reduction)\n",
      "Number of rows: 77613\n",
      "Number of columns: 2 \n",
      "\n",
      "Mem. usage decreased to 512.45 Mb (61.2% reduction)\n",
      "Number of rows: 2985217\n",
      "Number of columns: 57 \n",
      "\n",
      "Mem. usage decreased to 512.45 Mb (61.2% reduction)\n",
      "Number of rows: 2985217\n",
      "Number of columns: 57 \n",
      "\n",
      "Data successfully imported!\n"
     ]
    }
   ],
   "source": [
    "df_2016 = load_data('data/train_2016_v2.csv')\n",
    "df_2017 = load_data('data/train_2017.csv')\n",
    "prop_2016 = load_data('data/properties_2016.csv')\n",
    "prop_2017 = load_data('data/properties_2017.csv')\n",
    "assert len(prop_2016) == len(prop_2017)\n",
    "print('Data successfully imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11016594</td>\n",
       "      <td>0.028</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14366692</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12098116</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12643413</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2016-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14432541</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>2016-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  logerror transactiondate\n",
       "0  11016594     0.028      2016-01-01\n",
       "1  14366692    -0.168      2016-01-01\n",
       "2  12098116    -0.004      2016-01-01\n",
       "3  12643413     0.022      2016-01-02\n",
       "4  14432541    -0.005      2016-01-02"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>architecturalstyletypeid</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingclasstypeid</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>decktypeid</th>\n",
       "      <th>...</th>\n",
       "      <th>numberofstories</th>\n",
       "      <th>fireplaceflag</th>\n",
       "      <th>structuretaxvaluedollarcnt</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <th>taxamount</th>\n",
       "      <th>taxdelinquencyflag</th>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <th>censustractandblock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000e+00</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.752e+04</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>27516.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>650756.0</td>\n",
       "      <td>1.413e+06</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>762631.0</td>\n",
       "      <td>20800.369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>571346.0</td>\n",
       "      <td>1.157e+06</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>585488.0</td>\n",
       "      <td>14557.570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193796.0</td>\n",
       "      <td>4.335e+05</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>239695.0</td>\n",
       "      <td>5725.170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  airconditioningtypeid  architecturalstyletypeid  basementsqft  \\\n",
       "0  10754147                    NaN                       NaN           NaN   \n",
       "1  10759547                    NaN                       NaN           NaN   \n",
       "2  10843547                    NaN                       NaN           NaN   \n",
       "3  10859147                    NaN                       NaN           NaN   \n",
       "4  10879947                    NaN                       NaN           NaN   \n",
       "\n",
       "   bathroomcnt  bedroomcnt  buildingclasstypeid  buildingqualitytypeid  \\\n",
       "0          0.0         0.0                  NaN                    NaN   \n",
       "1          0.0         0.0                  NaN                    NaN   \n",
       "2          0.0         0.0                  NaN                    NaN   \n",
       "3          0.0         0.0                  3.0                    7.0   \n",
       "4          0.0         0.0                  4.0                    NaN   \n",
       "\n",
       "   calculatedbathnbr  decktypeid  ...  numberofstories  fireplaceflag  \\\n",
       "0                NaN         NaN  ...              NaN            NaN   \n",
       "1                NaN         NaN  ...              NaN            NaN   \n",
       "2                NaN         NaN  ...              NaN            NaN   \n",
       "3                NaN         NaN  ...              1.0            NaN   \n",
       "4                NaN         NaN  ...              NaN            NaN   \n",
       "\n",
       "   structuretaxvaluedollarcnt  taxvaluedollarcnt  assessmentyear  \\\n",
       "0                         NaN          9.000e+00          2015.0   \n",
       "1                         NaN          2.752e+04          2015.0   \n",
       "2                    650756.0          1.413e+06          2015.0   \n",
       "3                    571346.0          1.157e+06          2015.0   \n",
       "4                    193796.0          4.335e+05          2015.0   \n",
       "\n",
       "   landtaxvaluedollarcnt  taxamount  taxdelinquencyflag  taxdelinquencyyear  \\\n",
       "0                    9.0        NaN                 NaN                 NaN   \n",
       "1                27516.0        NaN                 NaN                 NaN   \n",
       "2               762631.0  20800.369                 NaN                 NaN   \n",
       "3               585488.0  14557.570                 NaN                 NaN   \n",
       "4               239695.0   5725.170                 NaN                 NaN   \n",
       "\n",
       "   censustractandblock  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Engineering\n",
    "After loading the data, we can now continue on the feature engineering part. The engineering part will be divided into the following steps:\n",
    "\n",
    "1. Extract datetime features\n",
    "2. Capture missing pattern\n",
    "3. Transfrom categorical features\n",
    "4. Define other features specific to the problem\n",
    "5. Drop unused columns\n",
    "\n",
    "### Extract Datetime\n",
    "Whenever there are datetime features in the dataset, it can be valuable to extract those datetime values. But we can take a step back and think for a while -- why datetime matters in this problem?\n",
    "\n",
    "It is because house prices can fluctuate due to seasonality and business cycles. \n",
    "\n",
    "For example, in [Zillow's Home Buying Guide](https://www.zillow.com/home-buying-guide/best-time-to-buy-a-house/) it states that **the best month to buy a house is August. Generally speaking, buyers in the fall and winter will have fewer options yet more flexibility in price, and spring and summer buyers will have more options, but less negotiating power.**\n",
    "\n",
    "Let's move on to extract datetime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_date_features(df):\n",
    "    \"\"\"\n",
    "    Extract datetime related features\n",
    "    \"\"\"\n",
    "    ## Retrieve transaction datetime values\n",
    "    dt = pd.to_datetime(df['transactiondate']).dt\n",
    "    df['trans_year'] = (dt.year).astype(int)\n",
    "    df['trans_month'] = (dt.month).astype(int)\n",
    "    df['trans_quarter'] = (dt.quarter).astype(int)\n",
    "    return df\n",
    "\n",
    "df_2016 = get_transaction_date_features(df_2016)\n",
    "df_2017 = get_transaction_date_features(df_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop_datetime_features(df, features):\n",
    "    \"\"\"\n",
    "    Extract datetime related features\n",
    "    \"\"\" \n",
    "    ## Relationship between age of the house, assessment year, taxdelinquency record, etc.\n",
    "    ## Assume now is 2020\n",
    "    df['house_age'] = 2020 - df['yearbuilt']\n",
    "    df['years_after_assessment'] = 2020 - df['assessmentyear']\n",
    "    df['has_tax_delinquent'] = [1 if t > 0 else 0 for t in df['taxdelinquencyyear']]\n",
    "    return(df)\n",
    "\n",
    "features = ['yearbuilt', 'assessmentyear','taxdelinquencyyear']\n",
    "prop_2016 = get_prop_datetime_features(prop_2016, features)\n",
    "prop_2017 = get_prop_datetime_features(prop_2017, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value \n",
    "\n",
    "Inspired by this [kaggle post](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108727) and multiple medium posts, we know that we can learn a lot of the missing value patterns. Again, why in reality missing value can be helpful?\n",
    "\n",
    "It is because **the more missing value you have, the less likely customers as well as Zillow can assess properties' real value accurately**. Some values might be more important than others when it comes to prediction accuracy. \n",
    "\n",
    "To capture missing value patterns, we can use PCA algorithm to get the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To determine number of components that best capture the missing value patterns.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_components(df_list, threshold = 0.8):\n",
    "    \"\"\"\n",
    "    Get number of components to feed in for PCA.\n",
    "    We need to make sure that two dataframes have the same embedding.\n",
    "    One quick way to ensure this is to take the maximum of the two suggested components.\n",
    "    \n",
    "    np.where () >>> return a list of indexes for True values\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for df in df_list:\n",
    "        n_components = df.shape[1] - 1\n",
    "        null_pattern = df.isnull().astype(int)\n",
    "        pca = PCA(n_components)\n",
    "        pca.fit(null_pattern)\n",
    "        index = np.where(np.cumsum(pca.explained_variance_ratio_) > threshold)[0][0] # the index where cumulative sum > threshold\n",
    "        result.append(index)\n",
    "    return np.max(result)\n",
    "\n",
    "n_components = get_components([prop_2016, prop_2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_pattern(df, n_components):\n",
    "    \"\"\"\n",
    "    Add null value embedding into the dataset\n",
    "    \"\"\"\n",
    "    ## Create column names\n",
    "    columns = []\n",
    "    for i, x in enumerate(range(n_components), start=1):\n",
    "        columns.append('NAPattern_' + str(i))\n",
    "    \n",
    "    ## Conduct PCA\n",
    "    null_pattern = df.isnull().astype(int)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    null_embedding = pd.DataFrame(pca.fit_transform(null_pattern), columns=columns)\n",
    "    \n",
    "    ## Update dataframe\n",
    "    df = pd.concat([df,null_embedding],axis=1)\n",
    "    return(df)\n",
    "\n",
    "prop_2016 = get_null_pattern(prop_2016, n_components)\n",
    "prop_2017 = get_null_pattern(prop_2017, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we capture the missing patterns, we can now impute missing values. Missing value imputation is more of an art than a science. We need to thoroughly think through each of the column definition and decide the best imputation techniques. \n",
    "\n",
    "### Impute Categorical Features\n",
    "\n",
    "For categorical features, the most widely used imputation techniques are **mode** or **unseen**. Mode means to impute missing values with the mode of the column, and Zero means to impute missing values with another value that is unseen in the column.\n",
    "\n",
    "In this project, there are lots of categorical features with missing values, for example\n",
    "\n",
    "```python\n",
    "['regionidzip','propertyzoningdesc','propertycountylandusecode','regionidcounty'\n",
    " ,'regionidcity','airconditioningtypeid','buildingqualitytypeid','fips'\n",
    " ,'heatingorsystemtypeid']\n",
    "```\n",
    "\n",
    "We will encode them along with the feature enginnering process. Here we first create a function to encode them into one of the unseen values so that we can groupby those categorical columns together afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical(df, feature):\n",
    "    \"\"\"\n",
    "    To impute categorical values based on strategy assigned. \n",
    "    \"\"\"\n",
    "    df.loc[df[feature].isnull(),feature] = -99\n",
    "    df[feature] = df[feature].astype('category')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-feature Engineering\n",
    "\n",
    "We know that houses are physical properties, and there are some popular neighborhood as well as notorious ones. Not only the neighborhood **(clusters)** matter, the distances to **the center of the cluster** can also be a factor. \n",
    "\n",
    "Bearing this in mind, we can take advantage of **KMeans** algorithm to extract some geographical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of null values: 11437\n",
      "Consist of 0.383% of the dataset. \n",
      "\n",
      "Total number of null values: 11437\n",
      "Consist of 0.383% of the dataset. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "detect_null(prop_2016.latitude)\n",
    "detect_null(prop_2016.longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first notice that there are missing values in both longitude and latitude series. Therefore, we have to impute those missing values so that we can get proper clusters. Since the missing values is relatively rare, we can simply impute them to the **mode** of all the possible longitudes and latitudes to even out the influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "geo_list = ['latitude', 'longitude']\n",
    "def conduct_geo_imputation(df, geo_list):\n",
    "    \"\"\"\n",
    "    Impute missing values by mode.\n",
    "    \"\"\"\n",
    "    for geo in geo_list:\n",
    "        df[geo] = df[geo].apply(lambda x: float(x))/10e6 ## shrink the data to reduct computational cost\n",
    "        df[geo] = df[geo].fillna(stats.mode(df[geo])[0][0])\n",
    "    return df\n",
    "\n",
    "prop_2016 = conduct_geo_imputation(prop_2016, geo_list)\n",
    "prop_2017 = conduct_geo_imputation(prop_2017, geo_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what we need to do is to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846 ns ± 31.4 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "prop_2016.name = 'prop_2016'\n",
    "prop_2017.name = 'prop_2017'\n",
    "def get_optimal_geocluster(df_list):\n",
    "    \"\"\"\n",
    "    Get the optimal value of clusters with the help of davies bouldin sore.\n",
    "    \"\"\"\n",
    "    K = range(5,16)\n",
    "    for df in df_list:\n",
    "        mapping_dav = {} \n",
    "\n",
    "        print(f'Start fitting ' + df.name + '.')\n",
    "        for k in K: \n",
    "            #Building and fitting the model \n",
    "            kmeanModel = KMeans(n_clusters=k)\n",
    "            pred = kmeanModel.fit_predict(df[geo_list])     \n",
    "\n",
    "            dav = davies_bouldin_score(df[geo_list], pred)\n",
    "            mapping_dav[k] = dav\n",
    "            if k % 5 == 0:\n",
    "                print(f\"Iteration {k} finished. The davies bouldin score for {k} cluster is: {np.round(dav,4)}\")\n",
    "        \n",
    "        ## The minimum davies bouldin score is zero, with lower values indicating better clustering.\n",
    "        print(f'The best 5 cluster number for ' + df.name + ' is '+ str(sorted(mapping_dav, key=mapping_dav.get, reverse=False)[:5]) + '\\n')\n",
    "\n",
    "# get_optimal_geocluster([prop_2016, prop_2017])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the lower the davie-bouldin index the better, we know that **k = 10~12** is the best number of clusters. Here let's use **k = 10** for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "\n",
    "prop_2016['geo_cluster'] = KMeans(n_clusters).fit_predict(prop_2016[geo_list])\n",
    "prop_2017['geo_cluster'] = KMeans(n_clusters).fit_predict(prop_2017[geo_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides clustering, we can also implement some useful transformation for geographical features. Let's take a step back to see what features we have on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parcelid', 'airconditioningtypeid', 'architecturalstyletypeid',\n",
       "       'basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingclasstypeid',\n",
       "       'buildingqualitytypeid', 'calculatedbathnbr', 'decktypeid',\n",
       "       'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet',\n",
       "       'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15',\n",
       "       'finishedsquarefeet50', 'finishedsquarefeet6', 'fips', 'fireplacecnt',\n",
       "       'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'hashottuborspa',\n",
       "       'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet',\n",
       "       'poolcnt', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7',\n",
       "       'propertycountylandusecode', 'propertylandusetypeid',\n",
       "       'propertyzoningdesc', 'rawcensustractandblock', 'regionidcity',\n",
       "       'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt',\n",
       "       'storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid',\n",
       "       'unitcnt', 'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt',\n",
       "       'numberofstories', 'fireplaceflag', 'structuretaxvaluedollarcnt',\n",
       "       'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n",
       "       'taxamount', 'taxdelinquencyflag', 'taxdelinquencyyear',\n",
       "       'censustractandblock', 'house_age', 'years_after_assessment',\n",
       "       'has_tax_delinquent', 'NAPattern_1', 'NAPattern_2', 'NAPattern_3',\n",
       "       'NAPattern_4', 'NAPattern_5', 'NAPattern_6', 'geo_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_2016.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For longitude and latitude, we already perform clustering analysis to define `geo_cluster`.\n",
    "We can take a step further and think what other considerations we have when looking for a proper residence.\n",
    "\n",
    "Below are some of the considerations:\n",
    "1. **Location**: whether it's at the north or south of the neighborhood\n",
    "2. **Facilities**: whether there are pool, yard, parking lot, etc.\n",
    "3. **Spaciousness**: how many rooms we have in total and per square feet?\n",
    "\n",
    "Let's implement one by one below.\n",
    "\n",
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_coord(df):\n",
    "    \"\"\"\n",
    "    Get rotated coordination to see where it locates (in absolute value)\n",
    "    \"\"\"\n",
    "    ## Perform some linear transformation\n",
    "    df['coord_1'] = df['latitude'] + df['longitude']\n",
    "    df['coord_2'] = df['latitude'] - df['longitude']\n",
    "    df['coord_3'] = df['latitude'] + 2 * df['longitude']\n",
    "    df['coord_4'] = df['latitude'] - 2 * df['longitude']\n",
    "    \n",
    "    ## See where the house locates with respect to the cener of the neighborhood.\n",
    "    center = (np.median(prop_2016['latitude']), np.median(prop_2017['longitude']))\n",
    "    df['coord_5'] = df['latitude'] - center[0]\n",
    "    df['coord_6'] = df['longitude'] - center[1]\n",
    "    df['coord_7'] = df['coord_5']**2\n",
    "    df['coord_8'] = df['coord_6']**2\n",
    "    return(df)\n",
    "\n",
    "\n",
    "prop_2016 = get_rotated_coord(prop_2016)    \n",
    "prop_2017 = get_rotated_coord(prop_2017) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect we can consider is **supply and demand**. Why housing prices in NYC and Silicon valley are expensive? It's because the demand is high. The feature related to this are\n",
    "\n",
    "```python\n",
    "['regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'fips']\n",
    "```\n",
    "\n",
    "We can first see the number of unique values for these four features to look for proper granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 186 unique values in the feature \"regionidcity\".\n",
      "There are 3 unique values in the feature \"regionidcounty\".\n",
      "There are 528 unique values in the feature \"regionidneighborhood\".\n",
      "There are 405 unique values in the feature \"regionidzip\".\n",
      "There are 3 unique values in the feature \"fips\".\n"
     ]
    }
   ],
   "source": [
    "region_list = ['regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'fips']\n",
    "for reg in region_list:\n",
    "    print(f'There are {len(prop_2016[reg].value_counts())} unique values in the feature \"{reg}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we have already construct a feature called **geo_cluster**, which we cluster all the houses within 10 clusters. It is more reasonable to choose a more granular feature -- in this case **regionidcity** is a better option. \n",
    "\n",
    "One quick way to consider supply and demand is to add a feature called **region_city_supply**, which takes in the number of values appeared in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the regioneighborhood has lots of missing value, here we use regionzip to define neighborhood instead \n",
    "def get_housing_supply(df):\n",
    "    df['region_city_supply'] = df['regionidcity'].map(df['regionidcity'].value_counts())\n",
    "    return(df)\n",
    "\n",
    "prop_2016 = get_housing_supply(prop_2016)    \n",
    "prop_2017 = get_housing_supply(prop_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "Before we actually dive into the numerical values, we need to note that since this is a regression problem, outliers can be really influential and/or destructive to the results. We can start by capping values so that the regression can make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_numerical_outliers(df, num_list):\n",
    "    \"\"\"\n",
    "    Remove numerical outliers in the dataset.\n",
    "    \"\"\"\n",
    "    for col in num_list:\n",
    "        df[col] = np.clip(df[col], np.nanmin(df[col]), np.nanpercentile(df[col], 95))\n",
    "    return df\n",
    "\n",
    "\n",
    "num_list = ['basementsqft','bathroomcnt','bedroomcnt','calculatedbathnbr', 'finishedfloor1squarefeet',\n",
    "            'calculatedfinishedsquarefeet', 'finishedsquarefeet12','finishedsquarefeet13','finishedsquarefeet15',\n",
    "            'finishedsquarefeet50','finishedsquarefeet6','fireplacecnt','fullbathcnt','garagecarcnt',\n",
    "            'garagetotalsqft','lotsizesquarefeet','poolcnt','poolsizesum','roomcnt','threequarterbathnbr',\n",
    "            'unitcnt','numberofstories','structuretaxvaluedollarcnt','taxvaluedollarcnt',\n",
    "            'landtaxvaluedollarcnt', 'taxamount']\n",
    "\n",
    "prop_2016 = cap_numerical_outliers(prop_2016, num_list)    \n",
    "prop_2017 = cap_numerical_outliers(prop_2017, num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facilities\n",
    "\n",
    "Let's consider facilities-related feature in this dataset. First we can take a look at pools.\n",
    "\n",
    "```python\n",
    "['poolcnt', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7']\n",
    "```\n",
    "As the definition defined in the data dictionary, we can conduct further mapping to create some useful features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool_features(df):\n",
    "    \"\"\"\n",
    "    Extract and construct pool-related features.\n",
    "    \"\"\"\n",
    "    ## whether or not there are pool / spa\n",
    "    df['has_pool'] = [1 if c >= 1 else 0 for c in df['poolcnt']]\n",
    "    df['has_spa'] = [1 if c >= 1 else 0 for c in df['pooltypeid10']]\n",
    "    df['has_pool_size'] = [1 if c > 0 else 0 for c in df['poolsizesum']]\n",
    "    df['pool_size_per_bedroom'] = [s/c if s > 0 and c > 0 else 0 for s, c in zip(prop_2016['poolsizesum'], prop_2016['bedroomcnt'])]\n",
    "    return df\n",
    "\n",
    "prop_2016 = get_pool_features(prop_2016) \n",
    "prop_2017 = get_pool_features(prop_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we can take a look at other features related to the uniqueness of the house.\n",
    "```python\n",
    "['airconditioningtypeid', 'architecturalstyletypeid', 'buildingqualitytypeid', 'buildingclasstypeid', 'decktypeid', 'storytypeid', 'propertycountylandusecode', 'propertylandusetypeid', 'propertyzoningdesc', 'rawcensustractandblock', \n",
    "'censustractandblock', 'typeconstructiontypeid'] \n",
    "```\n",
    "\n",
    "When we take a closer look, we can see that there are LOTS of missing values for these features. Some of them are only have one unique values. For these type of values, we have two possible procedures:\n",
    "\n",
    "1. Create an identity column (has_xxx) \n",
    "2. Impute missing values and encode later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical_features(df, features, rare_value):\n",
    "    \"\"\"\n",
    "    Trasnform categorical features based on their characteristics for later encoding.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        # Step 1: Create identity columns (note that missing value is -99)\n",
    "        df[f'has_{feature}'] = df[feature].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "        \n",
    "        # Step 2: Impute missing values\n",
    "        df = impute_categorical(df, feature)\n",
    "        \n",
    "        # Step 3: transform rare features\n",
    "        ids = df[feature].value_counts() / len(df) < 0.01\n",
    "        rare_index = list(ids[ids].index)\n",
    "        df[feature] = df[feature].apply(lambda x: rare_value if x in rare_index else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['buildingqualitytypeid', 'propertylandusetypeid',\n",
    " 'rawcensustractandblock', 'buildingclasstypeid',  'censustractandblock',\n",
    " 'architecturalstyletypeid', 'yearbuilt', 'propertyzoningdesc',\n",
    " 'airconditioningtypeid', 'regionidzip', 'assessmentyear', 'decktypeid', 'taxdelinquencyyear',\n",
    " 'heatingorsystemtypeid', 'storytypeid', 'regionidcity', 'fips', 'regionidneighborhood', 'censustractandblock',\n",
    " 'propertycountylandusecode', 'numberofstories', 'buildingclasstypeid', 'typeconstructiontypeid',\n",
    "               'regionidcounty']\n",
    "\n",
    "# arbitrarily defined simply for later grouping\n",
    "rare_value = -50\n",
    "\n",
    "prop_2016 = transform_categorical_features(prop_2016, cat_features, rare_value)\n",
    "prop_2017 = transform_categorical_features(prop_2017, cat_features, rare_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spaciousness & Others\n",
    "\n",
    "Intuitively the number of rooms per houses can be a critical factor for the value of house.\n",
    "\n",
    "There are multiple features related to spaciousness; for example, the number of rooms, the size of each room, basement, etc.\n",
    "\n",
    "There are also other features related to taxes. We will also covered those here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfrom_space(df):\n",
    "    \"\"\"\n",
    "    Transform space-related issues.\n",
    "    \"\"\"\n",
    "    ## 1. Identifier \n",
    "    df['has_finishedfloor1squarefeet'] = [1 if s >= 0 else 0 for s in df['finishedfloor1squarefeet']]\n",
    "    df['has_calculatedfinishedsquarefeet'] = [1 if s >= 0 else 0 for s in df['calculatedfinishedsquarefeet']]\n",
    "   \n",
    "    ## 2. Error\n",
    "    df['derived_squarefeet_error'] = df['calculatedfinishedsquarefeet'] - df['finishedfloor1squarefeet']\n",
    "    \n",
    "    ## 3. Impute Missing Values\n",
    "    df['calculatedfinishedsquarefeet'].fillna(0, inplace=True)\n",
    "    df['finishedfloor1squarefeet'].fillna(0, inplace=True)\n",
    "    df['derived_squarefeet_error'].fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "prop_2016 = transfrom_space(prop_2016)\n",
    "prop_2017 = transfrom_space(prop_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_division(df, numerator, denominator, new_feature_name):\n",
    "    df[new_feature_name] = df[numerator].fillna(0) / df[denominator].fillna(0)\n",
    "    df[new_feature_name] = df[new_feature_name].apply(lambda x: 0 if x == np.inf else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def transform_room_features(df):\n",
    "    \"\"\"\n",
    "    Transform room features based on the definition.\n",
    "    \"\"\"\n",
    "    ## 1. Bathroom: \n",
    "    ## After observation we can see that bathroomcnt = fullbathcnt + threequarterbathnbr\n",
    "    ## We can simply deop two features: bathroomcnt & calculatedbathnbr\n",
    "    df['threequarterbathnbr'] = df['threequarterbathnbr'].fillna(0)\n",
    "    df['fullbathcnt'] = df['fullbathcnt'].fillna(0)\n",
    "    df['bathroomcnt'] = df['threequarterbathnbr'] + df['fullbathcnt']\n",
    "    \n",
    "    ## 2. Bedroomcnt\n",
    "    df['bedroomcnt'] = df['fullbathcnt'].fillna(0)\n",
    "    \n",
    "    ## 3. Total room\n",
    "    df['roomcnt'] = df['roomcnt'].fillna(0)\n",
    "    \n",
    "    ## 4. Identifier for room & garage > there are listings with no rooms\n",
    "    df['has_room'] = [1 if r > 0 else 0 for r in df['roomcnt']]\n",
    "    df['has_garage'] = [1 if g > 0 else 0 for g in df['garagecarcnt']]\n",
    "    \n",
    "    ## 5. Error\n",
    "    df['derived_room_cnt'] = df['bedroomcnt'] + df['bathroomcnt']\n",
    "    df['diff_derived_roomcnt'] = df['bedroomcnt'] + df['bathroomcnt'] - df['roomcnt']\n",
    "    \n",
    "    ## 6. Relationship\n",
    "    df = robust_division(df, 'bathroomcnt','bedroomcnt','bathrm_to_bedrm')    \n",
    "    df = robust_division(df, 'threequarterbathnbr','fullbathcnt','half_bath_to_full_bath')    \n",
    "    df = robust_division(df, 'roomcnt','garagecarcnt','room_to_garage')    \n",
    "    \n",
    "    return df\n",
    "\n",
    "prop_2016 = transform_room_features(prop_2016)\n",
    "prop_2017 = transform_room_features(prop_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tax_feature(df):\n",
    "    \"\"\"\n",
    "    Transform tax related features\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Identifier\n",
    "    df['has_structuretaxvalue'] = [1 if tax > 0 else 0 for tax in df['structuretaxvaluedollarcnt']]\n",
    "    df['has_landtaxvalue'] = [1 if tax > 0 else 0 for tax in df['landtaxvaluedollarcnt']]\n",
    "    df['has_taxamount'] = [1 if tax > 0 else 0 for tax in df['taxamount']]\n",
    "    \n",
    "    \n",
    "    ## Impute Missing value\n",
    "    df['structuretaxvaluedollarcnt'].fillna(0, inplace=True)\n",
    "    df['landtaxvaluedollarcnt'].fillna(0, inplace=True)\n",
    "    df['taxvaluedollarcnt'].fillna(0, inplace=True)\n",
    "    df['taxamount'].fillna(0, inplace=True)\n",
    "    \n",
    "    ## Average Tax Rate \n",
    "    df = robust_division(df, 'taxamount', 'calculatedfinishedsquarefeet', 'yearly_tax_per_sqft')\n",
    "    df = robust_division(df, 'taxvaluedollarcnt','calculatedfinishedsquarefeet','tax_per_sqft')    \n",
    "    df = robust_division(df, 'landtaxvaluedollarcnt','calculatedfinishedsquarefeet','land_tax_per_sqft')    \n",
    "    return df\n",
    "\n",
    "prop_2016 = transform_tax_feature(prop_2016)\n",
    "prop_2017 = transform_tax_feature(prop_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_house_feature(df):\n",
    "\n",
    "    ## Average Size for each room for house\n",
    "    df = robust_division(df, 'calculatedfinishedsquarefeet','derived_room_cnt','avg_room_size')    \n",
    "    \n",
    "    ## Average Size for each garage  \n",
    "    df = robust_division(df, 'garagetotalsqft','garagecarcnt','avg_garage_size')\n",
    "    \n",
    "    ## Average Lot Size to Garage    \n",
    "    df = robust_division(df, 'lotsizesquarefeet','garagetotalsqft','lot_to_garage_size')    \n",
    "    \n",
    "    ## Average Basement Size per total size\n",
    "    df = robust_division(df, 'basementsqft','calculatedfinishedsquarefeet','basement_to_total')\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "    ## Average Basement Size \n",
    "\n",
    "prop_2016 = get_addition_house_feature(prop_2016)\n",
    "prop_2017 = get_addition_house_feature(prop_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding\n",
    "\n",
    "Now we have compiled a list of categorical features as well as numerical ones. It can be helpful if we try to calculate some aggregation across categories now understand the characteristics of each house, and then understand whether it is below or above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Processing.\n",
      "10 of combinations (27.78%) have been processed. \n",
      "20 of combinations (55.56%) have been processed. \n",
      "30 of combinations (83.33%) have been processed. \n",
      "Start Shrinking sizes.\n",
      "Mem. usage decreased to 432.73 Mb (65.5% reduction)\n",
      "Mem. usage decreased to 723.12 Mb (65.7% reduction)\n",
      "Start Merging.\n",
      "Done. \n",
      "\n",
      "Mem. usage decreased to 1133.08 Mb (0.0% reduction)\n",
      "Start Processing.\n",
      "10 of combinations (27.78%) have been processed. \n",
      "20 of combinations (55.56%) have been processed. \n",
      "30 of combinations (83.33%) have been processed. \n",
      "Start Shrinking sizes.\n",
      "Mem. usage decreased to 432.73 Mb (65.5% reduction)\n",
      "Mem. usage decreased to 723.12 Mb (65.7% reduction)\n",
      "Start Merging.\n",
      "Done. \n",
      "\n",
      "Mem. usage decreased to 1133.08 Mb (0.0% reduction)\n",
      "CPU times: user 1min 56s, sys: 52.5 s, total: 2min 49s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def add_aggregate_across_categories(df, groupby_cols, aggregate_cols):\n",
    "    print(\"Start Processing.\")\n",
    "    dataframe = df.copy(deep=True)   \n",
    "    df[aggregate_cols] = df[aggregate_cols].fillna(0)\n",
    "    df = df[groupby_cols + aggregate_cols]\n",
    "    total = len(groupby_cols) * len(aggregate_cols)\n",
    "    i = 0 \n",
    "    for g, a in product(groupby_cols, aggregate_cols):\n",
    "        i += 1\n",
    "        temp = df.groupby(g)[a].agg([np.mean]).reset_index()\n",
    "        temp.columns = [g, f'{a}_across_{g}']\n",
    "        temp[f'{a}_across_{g}_diff'] = temp[f'{a}_across_{g}'] - np.average(df[a])\n",
    "\n",
    "        df = df.merge(how='left', right=temp, on=f'{g}')\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{i} of combinations ({np.round(i/total*100,2)}%) have been processed. \")\n",
    "    print(\"Start Shrinking sizes.\")\n",
    "    ind = df.columns[~(df.columns.isin(groupby_cols) | df.columns.isin(aggregate_cols))]\n",
    "    df = reduce_mem_usage(df[ind])\n",
    "    dataframe = reduce_mem_usage(dataframe)\n",
    "    print(\"Start Merging.\")\n",
    "    dataframe = pd.concat([dataframe, df], axis = 1)\n",
    "    print(\"Done. \\n\")\n",
    "    return(reduce_mem_usage(dataframe))\n",
    "\n",
    "groupby_cols = ['airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid',\n",
    "                'propertycountylandusecode','propertylandusetypeid','propertyzoningdesc',\n",
    "                'rawcensustractandblock', 'regionidcity','regionidneighborhood','regionidzip',\n",
    "                'fips','yearbuilt']\n",
    "\n",
    "aggregate_cols = ['avg_room_size', 'derived_room_cnt','yearly_tax_per_sqft']\n",
    "\n",
    "prop_2016 = add_aggregate_across_categories(prop_2016, groupby_cols, aggregate_cols)\n",
    "prop_2017 = add_aggregate_across_categories(prop_2017, groupby_cols, aggregate_cols)\n",
    "# _ = add_aggregate_across_categories(prop_2016, groupby_cols, aggregate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_2016['censustractandblock'][0] == -99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_alignment(df):\n",
    "    filtered_cen = [v / 10e5 if v != -99 else -99 for v in df['censustractandblock'].astype(float)]\n",
    "    filtered_rawcen = df['rawcensustractandblock'].astype(float)\n",
    "    df['cenblock'] = [1 if c > r else 0 for c, r in zip(filtered_cen, filtered_rawcen)]\n",
    "    return df\n",
    "\n",
    "prop_2016 = get_census_alignment(prop_2016)\n",
    "prop_2017 = get_census_alignment(prop_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_all_columns(df, impute_list):\n",
    "    \"\"\"\n",
    "    Impute missing values and transform other columns\n",
    "    \"\"\"\n",
    "    # Imputation\n",
    "    df[impute_list] = df[impute_list].fillna(0)\n",
    "    \n",
    "    \n",
    "    # Transformation\n",
    "    df['hashottuborspa'] = [1 if h == True else 0 for h in df['hashottuborspa']]\n",
    "    df['fireplaceflag'] = [1 if h == True else 0 for h in df['fireplaceflag']]\n",
    "    df['taxdelinquencyflag'] = [1 if h == 'Y' else 0 for h in df['taxdelinquencyflag']]\n",
    "    return df\n",
    "\n",
    "impute_list = ['basementsqft', 'fireplacecnt', 'garagecarcnt', 'garagetotalsqft',\n",
    "               'lotsizesquarefeet', 'unitcnt', 'yardbuildingsqft17', 'yardbuildingsqft26',\n",
    "               'house_age', 'years_after_assessment', 'region_city_supply',\n",
    "               'bathrm_to_bedrm', 'half_bath_to_full_bath', 'room_to_garage', \n",
    "               'yearly_tax_per_sqft', 'tax_per_sqft', 'land_tax_per_sqft',\n",
    "               'avg_room_size', 'avg_garage_size', 'lot_to_garage_size', 'basement_to_total',\n",
    "               'poolcnt', 'poolsizesum', 'finishedsquarefeet50']\n",
    "\n",
    "prop_2016 = cleanup_all_columns(prop_2016, impute_list)\n",
    "prop_2017 = cleanup_all_columns(prop_2017, impute_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 985.04 Mb (7.5% reduction)\n",
      "Mem. usage decreased to 985.04 Mb (7.5% reduction)\n",
      "Number of properties: 2985217\n",
      "Number of property features: 179\n",
      "CPU times: user 7min 23s, sys: 13.7 s, total: 7min 36s\n",
      "Wall time: 7min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Drop unneeded columns\n",
    "drop_list = ['latitude', 'longitude', 'taxdelinquencyyear', \n",
    "             'calculatedbathnbr','decktypeid','finishedfloor1squarefeet', \n",
    "             'finishedsquarefeet12', 'finishedsquarefeet13',\n",
    "             'finishedsquarefeet15', 'finishedsquarefeet6', 'fullbathcnt',  \n",
    "             'censustractandblock', 'rawcensustractandblock',\n",
    "             'calculatedbathnbr', 'pooltypeid10', 'pooltypeid2', \n",
    "             'pooltypeid7']\n",
    "\n",
    "prop_2016 = prop_2016.drop(columns=drop_list)\n",
    "prop_2017 = prop_2017.drop(columns=drop_list)\n",
    "\n",
    "# Reduce Size\n",
    "prop_2016 = reduce_mem_usage(prop_2016)\n",
    "prop_2017 = reduce_mem_usage(prop_2017)\n",
    "\n",
    "print(f\"Number of properties: {len(prop_2016)}\".format(len(prop_2016)))\n",
    "print(f\"Number of property features: {len(prop_2016.columns)-1}\")\n",
    "\n",
    "# Write feature DataFrames to csv\n",
    "prop_2016.to_csv('data/prop_2016_v2.csv', index=False)\n",
    "prop_2017.to_csv('data/prop_2017_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 985.04 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 985.04 Mb (0.0% reduction)\n",
      "Number of properties: 2985217\n",
      "Number of property features: 179\n"
     ]
    }
   ],
   "source": [
    "# Reduce Size\n",
    "prop_2016 = reduce_mem_usage(prop_2016)\n",
    "prop_2017 = reduce_mem_usage(prop_2017)\n",
    "\n",
    "print(f\"Number of properties: {len(prop_2016)}\".format(len(prop_2016)))\n",
    "print(f\"Number of property features: {len(prop_2016.columns)-1}\")\n",
    "\n",
    "# Write feature DataFrames to csv\n",
    "prop_2016.to_csv('data/prop_2016_v2.csv', index=False)\n",
    "prop_2017.to_csv('data/prop_2017_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training set size: 167888\n"
     ]
    }
   ],
   "source": [
    "# Merge Dataset\n",
    "df_prop_2016 = pd.merge(df_2016,prop_2016,how='left',on='parcelid')\n",
    "df_prop_2017 = pd.merge(df_2017,prop_2017,how='left',on='parcelid')\n",
    "\n",
    "# Combine the 2016 and 2017 training sets\n",
    "train = pd.concat([df_prop_2016, df_prop_2017], axis=0, ignore_index=True)\n",
    "print(f\"Combined training set size: {len(train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 57.64 Mb (5.3% reduction)\n",
      "CPU times: user 13.6 s, sys: 380 ms, total: 13.9 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reduce Size\n",
    "train = reduce_mem_usage(train)\n",
    "\n",
    "# Write training DataFrame to CSV\n",
    "train.to_csv('data/train_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
